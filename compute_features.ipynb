{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588d210d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from PIL import Image\n",
    "\n",
    "import time\n",
    "\n",
    "import torch.utils.data as data_utils\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch import nn\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "\n",
    "from torchvision.datasets import CIFAR100\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "\n",
    "from scipy.io import loadmat\n",
    "from scipy.io import savemat\n",
    "\n",
    "try:\n",
    "    from torchvision.transforms import InterpolationMode\n",
    "    BICUBIC = InterpolationMode.BICUBIC\n",
    "except ImportError:\n",
    "    BICUBIC = Image.BICUBIC\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f587c5d-3616-43a5-9993-839ec4299703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "def ParseAnoXml(path):\n",
    "    files = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "    classes={}\n",
    "    nf=len(files)\n",
    "    for i in range(nf):\n",
    "        f=files[i]\n",
    "        tree = ET.parse(path+f)\n",
    "        root = tree.getroot()\n",
    "        child=root.findall(\"./object/name\")[0]\n",
    "        fj=f[0:-4]+'.jpeg'\n",
    "        if child.text in classes.keys():\n",
    "            classes[child.text].append(fj)\n",
    "        else:\n",
    "            classes[child.text]=[fj]\n",
    "        if i%1000==0:\n",
    "            print(i,fj)\n",
    "    return classes\n",
    "path='C:/Datasets/ILSVRC2016/ILSVRC/Annotations/CLS-LOC/val/'\n",
    "classes=ParseAnoXml(path)\n",
    "torch.save(classes,'C:/Datasets/ILSVRC2016/classes_val.pth')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4768a2-8355-4bdf-b382-9e572b4270ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The CLIP model\n",
    "import clip\n",
    "\n",
    "def _transform(n_px):\n",
    "    return Compose([\n",
    "        Resize(n_px, interpolation=BICUBIC),\n",
    "        CenterCrop(n_px),\n",
    "        _convert_image_to_rgb,\n",
    "        ToTensor(),\n",
    "        Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "    ])\n",
    "\n",
    "def features(net, x):\n",
    "    #print(0,x.shape)\n",
    "    x = x.type(net.conv1.weight.dtype)\n",
    "    for conv, bn in [(net.conv1, net.bn1), (net.conv2, net.bn2), (net.conv3, net.bn3)]:\n",
    "        x = net.relu(bn(conv(x)))\n",
    "    x = net.avgpool(x)\n",
    "    x = net.layer1(x)\n",
    "    x = net.layer2(x)\n",
    "    x = net.layer3(x)\n",
    "    x = net.layer4(x)\n",
    "    x = net.attnpool(x)\n",
    "    #x=F.avg_pool2d(x,x.shape[2],1) For CIFAR\n",
    "    return x\n",
    "\n",
    "print(clip.available_models())\n",
    "model, preprocess = clip.load('RN50x4', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccc2b64-82e4-4603-8e14-14849db1dd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The SWSL Model from \n",
    "# Billion-scale semi-supervised learning for image classification, https://arxiv.org/abs/1905.00546\n",
    "\n",
    "def features(net, x):\n",
    "    # See note [TorchScript super()]\n",
    "    x = net.conv1(x)\n",
    "    x = net.bn1(x)\n",
    "    x = net.relu(x)\n",
    "    x = net.maxpool(x)\n",
    "\n",
    "    x = net.layer1(x)\n",
    "    x = net.layer2(x)\n",
    "    x = net.layer3(x)\n",
    "    x = net.layer4(x)\n",
    "    \n",
    "    #print(x.shape)\n",
    "    #x = net.avgpool(x)\n",
    "    x = F.avg_pool2d(x,x.shape[2],1)\n",
    "    x = torch.flatten(x, 1)\n",
    "    return x\n",
    "\n",
    "def _transform(n_px):\n",
    "    return Compose([\n",
    "        Resize(n_px, interpolation=BICUBIC),\n",
    "        CenterCrop(n_px),\n",
    "        _convert_image_to_rgb,\n",
    "        ToTensor(),\n",
    "        Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ])\n",
    "\n",
    "torch.hub.list('facebookresearch/semi-supervised-ImageNet1K-models')\n",
    "model = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models', 'resnet50_swsl')\n",
    "model=model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc36d5f-0465-494a-b82e-f1270da6fff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitl14_reg')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d585a37-8886-485d-a01f-240c89abdf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path='d:/datasets/Imagenet/'\n",
    "path = 'd:/datasets/ILSVRC2016/'\n",
    "name = path+'train.txt'\n",
    "with open(name, 'r') as f:\n",
    "    names = f.read().splitlines()\n",
    "nc=len(names)\n",
    "print(nc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf56b07-443d-4e11-8250-90b7b49348e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute features Imagenet and save them\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset,TensorDataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from typing import Sequence\n",
    "\n",
    "# class GaussianBlur(transforms.RandomApply):\n",
    "#     \"\"\"\n",
    "#     Apply Gaussian Blur to the PIL image.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, *, p: float = 0.5, radius_min: float = 0.1, radius_max: float = 2.0):\n",
    "#         # NOTE: torchvision is applying 1 - probability to return the original image\n",
    "#         keep_p = 1 - p\n",
    "#         transform = transforms.GaussianBlur(kernel_size=9, sigma=(radius_min, radius_max))\n",
    "#         super().__init__(transforms=[transform], p=keep_p)\n",
    "\n",
    "class MaybeToTensor(transforms.ToTensor):\n",
    "    \"\"\"\n",
    "    Convert a ``PIL Image`` or ``numpy.ndarray`` to tensor, or keep as is if already a tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, pic):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pic (PIL Image, numpy.ndarray or torch.tensor): Image to be converted to tensor.\n",
    "        Returns:\n",
    "            Tensor: Converted image.\n",
    "        \"\"\"\n",
    "        if isinstance(pic, torch.Tensor):\n",
    "            return pic\n",
    "        return super().__call__(pic)\n",
    "\n",
    "\n",
    "# Use timm's names\n",
    "IMAGENET_DEFAULT_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_DEFAULT_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "def make_normalize_transform(\n",
    "    mean: Sequence[float] = IMAGENET_DEFAULT_MEAN,\n",
    "    std: Sequence[float] = IMAGENET_DEFAULT_STD,\n",
    ") -> transforms.Normalize:\n",
    "    return transforms.Normalize(mean=mean, std=std)\n",
    "\n",
    "# This roughly matches torchvision's preset for classification training:\n",
    "#   https://github.com/pytorch/vision/blob/main/references/classification/presets.py#L6-L44\n",
    "def make_classification_train_transform(\n",
    "    *,\n",
    "    crop_size: int = 224,\n",
    "    interpolation=transforms.InterpolationMode.BICUBIC,\n",
    "    hflip_prob: float = 0.5,\n",
    "    mean: Sequence[float] = IMAGENET_DEFAULT_MEAN,\n",
    "    std: Sequence[float] = IMAGENET_DEFAULT_STD,\n",
    "):\n",
    "    transforms_list = [transforms.RandomResizedCrop(crop_size, interpolation=interpolation)]\n",
    "    if hflip_prob > 0.0:\n",
    "        transforms_list.append(transforms.RandomHorizontalFlip(hflip_prob))\n",
    "    transforms_list.extend(\n",
    "        [\n",
    "            MaybeToTensor(),\n",
    "            make_normalize_transform(mean=mean, std=std),\n",
    "        ]\n",
    "    )\n",
    "    return transforms.Compose(transforms_list)\n",
    "\n",
    "\n",
    "# This matches (roughly) torchvision's preset for classification evaluation:\n",
    "#   https://github.com/pytorch/vision/blob/main/references/classification/presets.py#L47-L69\n",
    "def make_classification_eval_transform(\n",
    "    *,\n",
    "    resize_size: int = 256,\n",
    "    interpolation=transforms.InterpolationMode.BICUBIC,\n",
    "    crop_size: int = 224,\n",
    "    mean: Sequence[float] = IMAGENET_DEFAULT_MEAN,\n",
    "    std: Sequence[float] = IMAGENET_DEFAULT_STD,\n",
    ") -> transforms.Compose:\n",
    "    transforms_list = [\n",
    "        transforms.Resize(resize_size, interpolation=interpolation),\n",
    "        transforms.CenterCrop(crop_size),\n",
    "        MaybeToTensor(),\n",
    "        make_normalize_transform(mean=mean, std=std),\n",
    "    ]\n",
    "    return transforms.Compose(transforms_list)\n",
    "    \n",
    "def LoadImages(transform,path,files): \n",
    "    n=len(files)\n",
    "    x=[]\n",
    "    for i in range(n):\n",
    "        im = Image.open(join(path, files[i]))\n",
    "        #print(i,files[i])\n",
    "        x.append(transform(im.convert(\"RGB\")))\n",
    "    x=torch.stack(x)\n",
    "    print(x.shape)\n",
    "    return x\n",
    "\n",
    "def ComputeFeatures(model,x,batch_size=16): \n",
    "    data=TensorDataset(x)\n",
    "    loader=DataLoader(data,batch_size=batch_size,shuffle=False)\n",
    "    X=[]\n",
    "    for images in loader:\n",
    "        images=images[0].to(device)\n",
    "        with torch.no_grad():\n",
    "            fi=model(images)   \n",
    "        X.append(fi.cpu())\n",
    "        #print(len(X),fi.shape)\n",
    "    X=torch.cat(X,dim=0)\n",
    "    return X.squeeze()\n",
    "\n",
    "model=dinov2_vitl14_reg.to(device)\n",
    "preprocess = make_classification_eval_transform()\n",
    "for i in range(426,nc):\n",
    "    path='d:/Datasets/ILSVRC2016/ILSVRC/Data/CLS-LOC/train/'+names[i]\n",
    "    files = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "    x=LoadImages(preprocess,path,files)\n",
    "    x=ComputeFeatures(dinov2_vitl14_reg,x.to(device),256)\n",
    "    name='d:/Datasets/ILSVRC2016/dinov2_vitl14_reg/'+names[i]+'.mat'\n",
    "    print(i,name)\n",
    "    savemat(name,{'feature':x.float().numpy()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f3f2bb-49c5-4f48-b3eb-55e339483d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=torch.load('d:/Datasets/ILSVRC2016/classes_val0.pth')\n",
    "path='d:/Datasets/ILSVRC2016/ILSVRC/Data/CLS-LOC/val/'\n",
    "for i in range(nc):\n",
    "    files = classes[names[i]]\n",
    "    x=LoadImages(preprocess,path,files)\n",
    "    x=ComputeFeatures(dinov2_vitl14_reg,x.to(device),256)\n",
    "    name='d:/Datasets/ILSVRC2016/dinov2_vitl14_reg_val/'+names[i]+'.mat'\n",
    "    print(i,name)\n",
    "    savemat(name,{'feature':x.float().numpy()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2b8bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_image_to_rgb(image):\n",
    "    return image.convert(\"RGB\")\n",
    "\n",
    "def _transform(n_px):\n",
    "    return Compose([\n",
    "        Resize(n_px, interpolation=BICUBIC),\n",
    "        CenterCrop(n_px),\n",
    "        _convert_image_to_rgb,\n",
    "        ToTensor(),\n",
    "        Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "    ])\n",
    "nx=144\n",
    "\n",
    "import clip\n",
    "print(clip.available_models())\n",
    "model, preprocess = clip.load('RN50x4', device)\n",
    "\n",
    "relu = torch.nn.functional.relu\n",
    "def features(net, x):\n",
    "    x = x.type(net.conv1.weight.dtype)\n",
    "    for conv, bn in [(net.conv1, net.bn1), (net.conv2, net.bn2), (net.conv3, net.bn3)]:\n",
    "        x = relu(bn(conv(x)))\n",
    "    x = net.avgpool(x)\n",
    "    x = net.layer1(x)\n",
    "    x = net.layer2(x)\n",
    "    x = net.layer3(x)\n",
    "    x = net.layer4(x)\n",
    "    x=net.avgpool(x) \n",
    "    x=net.avgpool(x)\n",
    "    return x\n",
    "\n",
    "# Download the dataset\n",
    "cifar100_train = CIFAR100(root=os.path.expanduser(\"~/.cache\"), download=True, train=True, transform=_transform(nx))\n",
    "cifar100_test = CIFAR100(root=os.path.expanduser(\"~/.cache\"), download=True, train=False, transform=_transform(nx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94681d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def generate_features(dataset,model):\n",
    "    labels = torch.empty(0).cpu()\n",
    "    i = 0\n",
    "    for images, labs in tqdm(DataLoader(dataset, batch_size=100)):\n",
    "            images = images.to(device)\n",
    "            labs = labs.cpu()\n",
    "            with torch.no_grad():\n",
    "                f = features(model.visual,images) \n",
    "                f = f.squeeze((2,3))\n",
    "            f=f.cpu()\n",
    "            if i==0:\n",
    "                d=f.shape[1]\n",
    "                feat = torch.empty(0, d).cpu()\n",
    "            feat = torch.cat((feat,f),dim=0)\n",
    "            labels = torch.cat((labels , labs),dim=0)\n",
    "            i = i+1\n",
    "    return feat, labels\n",
    "\n",
    "features_train,labels_train = generate_features(cifar100_train,model)\n",
    "features_test,labels_test = generate_features(cifar100_test,model)\n",
    "\n",
    "for i in range(100):\n",
    "    name = r'D:\\datasets\\Cifar100\\Clip\\train'+'{}'.format(i) +'.mat'\n",
    "    x = features_train[labels_train == i,:]\n",
    "    savemat(name,{'feature':x.float().numpy()})\n",
    "    name = r'D:\\datasets\\Cifar100\\Clip\\val'+'{}'.format(i) +'.mat'\n",
    "    x = features_test[labels_test == i,:]\n",
    "    savemat(name,{'feature':x.float().numpy()})\n",
    "    if i%10==0:\n",
    "        print(i)\n",
    "print('All features saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35238d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
